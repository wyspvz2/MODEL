#  Transformer Encoder çš„æ•°å­¦è¿‡ç¨‹å’Œä»£ç ç¤ºä¾‹

---

##  ä¸€ã€æ€»ä½“ç›®æ ‡

Transformer Encoder çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

> å°†è¾“å…¥åºåˆ—ï¼ˆå¦‚å¥å­æˆ–å›¾åƒ patch å‘é‡ï¼‰è½¬åŒ–ä¸ºå…¨å±€ä¸Šä¸‹æ–‡ç›¸å…³çš„é«˜å±‚è¯­ä¹‰è¡¨ç¤ºã€‚

æ¢å¥è¯è¯´ï¼š  
æ¯ä¸ªä½ç½®ï¼ˆå•è¯ / patchï¼‰æœ€ç»ˆéƒ½èƒ½â€œç†è§£â€æ•´ä¸ªåºåˆ—çš„è¯­ä¹‰å…³ç³»ã€‚

---

##  äºŒã€è¾“å…¥å½¢å¼

è¾“å…¥é€šå¸¸æ˜¯åµŒå…¥å‘é‡åºåˆ—ï¼š

$$X = [x_1, x_2, \dots, x_N] \in \mathbb{R}^{N \times D}$$

å…¶ä¸­ï¼š
- $N$ï¼šåºåˆ—é•¿åº¦ï¼ˆNLP ä¸­æ˜¯è¯æ•°ï¼ŒViT ä¸­æ˜¯ patch æ•°ï¼‰
- $D$ï¼šembedding ç»´åº¦ï¼ˆå¦‚ 512ã€768ï¼‰

å¦‚æœæ˜¯ ViTï¼Œè¿˜éœ€è¦ï¼š
1. æ·»åŠ ä¸€ä¸ª `[CLS]` tokenï¼š
   $$X' = [x_{cls}; x_1; x_2; \dots; x_N]$$
2. åŠ ä¸Šä½ç½®ç¼–ç ï¼š
   $$Z_0 = X' + E_{pos}$$

 **ç›®çš„ï¼š**
- `[CLS]` ç”¨äºæ±‡èšå…¨å±€ç‰¹å¾ã€‚
- ä½ç½®ç¼–ç  $E_{pos}$ ä¿ç•™åºåˆ—é¡ºåºä¿¡æ¯ï¼ˆå› ä¸ºæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«æ˜¯æ— åºçš„ï¼‰ã€‚

---

##  ä¸‰ã€Encoder çš„å±‚ç»“æ„

æ¯ä¸€å±‚ï¼ˆLayerï¼‰åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š

1. **å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰**
2. **å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰**

å¹¶ä¸”éƒ½åŒ…å«ï¼š
- æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰
- å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰

---

##  å››ã€Self-Attention çš„æ•°å­¦è¿‡ç¨‹ä¸ç›®çš„

### Step 1ï¸: çº¿æ€§æ˜ å°„ï¼ˆæ„é€  Q, K, Vï¼‰

å¯¹è¾“å…¥ $Z_{l-1}$ åšä¸‰æ¬¡çº¿æ€§å˜æ¢ï¼š

$$Q = Z_{l-1} W_Q, \quad K = Z_{l-1} W_K, \quad V = Z_{l-1} W_V$$

å…¶ä¸­ï¼š
- $W_Q, W_K, W_V \in \mathbb{R}^{D \times D_h}$
- $D_h = D / h$ æ˜¯å•ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
- $h$ï¼šæ³¨æ„åŠ›å¤´çš„æ•°é‡

 **ç›®çš„ï¼š**
- $Q$ï¼ˆQueryï¼‰è¡¨ç¤ºâ€œæˆ‘æƒ³å…³æ³¨ä»€ä¹ˆâ€
- $K$ï¼ˆKeyï¼‰è¡¨ç¤ºâ€œæˆ‘èƒ½æä¾›ä»€ä¹ˆâ€
- $V$ï¼ˆValueï¼‰è¡¨ç¤ºâ€œæˆ‘åŒ…å«çš„ä¿¡æ¯å†…å®¹â€

---

### Step 2ï¸: ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆQuery ä¸ Keyï¼‰

è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µï¼š

$$S = \frac{Q K^T}{\sqrt{D_h}}$$

ç„¶åè¿›è¡Œ softmax å½’ä¸€åŒ–ï¼š

$$A = \text{softmax}(S)$$

 **ç›®çš„ï¼š**
- è¡¡é‡æ¯ä¸ª token ä¸å…¶å®ƒ token çš„ç›¸å…³æ€§ï¼›
- $\sqrt{D_h}$ é˜²æ­¢å†…ç§¯å€¼è¿‡å¤§ï¼Œç¨³å®šæ¢¯åº¦ï¼›
- softmax ä½¿å¾—æƒé‡åœ¨ [0,1] ä¹‹é—´ä¸”å¯è§£é‡Šä¸ºâ€œæ³¨æ„åŠ›åˆ†å¸ƒâ€ã€‚

---

### Step 3ï¸: åŠ æƒæ±‚å’Œï¼ˆæ ¹æ®æ³¨æ„åŠ›èšåˆä¿¡æ¯ï¼‰

$$Z' = A V$$

 **ç›®çš„ï¼š**
æ¯ä¸ª token å¾—åˆ°æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯åŠ æƒæ±‡æ€»ã€‚  
å³æ¯ä¸ªä½ç½®â€œçœ‹åˆ°äº†â€å…¶å®ƒæ‰€æœ‰ä½ç½®çš„å†…å®¹ã€‚

---

### Step 4ï¸: å¤šå¤´æœºåˆ¶ï¼ˆMulti-Headï¼‰

$$\text{MHA}(Z) = [Z'_1; Z'_2; \dots; Z'_h] W_O$$

 **ç›®çš„ï¼š**
ä¸åŒçš„æ³¨æ„åŠ›å¤´åœ¨å…³æ³¨ä¸åŒçš„è¯­ä¹‰å…³ç³»ï¼ˆå±€éƒ¨ã€å…¨å±€ã€é¢œè‰²ã€å½¢çŠ¶ã€ä¸Šä¸‹æ–‡ç­‰ï¼‰ã€‚

---

```python

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # QKV çº¿æ€§å±‚
        self.W_Q = nn.Linear(embed_dim, embed_dim)
        self.W_K = nn.Linear(embed_dim, embed_dim)
        self.W_V = nn.Linear(embed_dim, embed_dim)
        self.W_O = nn.Linear(embed_dim, embed_dim)
        
    def forward(self, x):
        B, N, D = x.shape  # batch, seq_len, embed_dim
        
        # ç”Ÿæˆ Q, K, V
        Q = self.W_Q(x)  # [B, N, D]
        K = self.W_K(x)
        V = self.W_V(x)
        
        # åˆ†å¤´
        Q = Q.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # [B, h, N, D_h]
        K = K.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, h, N, N]
        attn_weights = torch.softmax(attn_scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attn_weights, V)  # [B, h, N, D_h]
        
        # åˆå¹¶å¤šå¤´
        out = out.transpose(1, 2).contiguous().view(B, N, D)  # [B, N, D]
        out = self.W_O(out)
        return out

```

##  äº”ã€æ®‹å·®è¿æ¥ä¸å½’ä¸€åŒ–

æ¯ä¸€å±‚éƒ½ä¼šåŠ ä¸Šè¾“å…¥çš„æ®‹å·®ï¼Œå¹¶åš LayerNormï¼š

$$Z'_l = \text{LayerNorm}(Z_{l-1} + \text{MHA}(Z_{l-1}))$$

 **ç›®çš„ï¼š**
- æ®‹å·®è¿æ¥ä¿è¯ä¿¡æ¯æµé€šã€é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼›
- LayerNorm ç¨³å®šè®­ç»ƒï¼Œä½¿åˆ†å¸ƒå¹³è¡¡ã€‚

---

## ğŸ”§ å…­ã€å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰

å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åœ°åšéçº¿æ€§å˜æ¢ï¼š

$$\text{FFN}(x) = \text{GELU}(x W_1 + b_1) W_2 + b_2$$

$$Z_l = \text{LayerNorm}(Z'_l + \text{FFN}(Z'_l))$$

 **ç›®çš„ï¼š**
- å¯¹æ¯ä¸ª token è¿›è¡Œæ›´å¤æ‚çš„ç‰¹å¾æ˜ å°„ï¼›
- å¢åŠ éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ï¼›
- ç¬¬äºŒæ¬¡æ®‹å·®ä¿è¯ç‰¹å¾ä¸ä¸¢å¤±ã€‚

```python
class FeedForward(nn.Module):
    def __init__(self, embed_dim, hidden_dim):
        super().__init__()
        self.fc1 = nn.Linear(embed_dim, hidden_dim)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden_dim, embed_dim)
        
    def forward(self, x):
        return self.fc2(self.act(self.fc1(x)))
```
---

##  ä¸ƒã€æ•°å­¦æµç¨‹æ€»ç»“

å®Œæ•´çš„ç¬¬ $l$ å±‚ Encoder å…¬å¼ï¼š

$$
\begin{aligned}
Q &= Z_{l-1} W_Q, \quad K = Z_{l-1} W_K, \quad V = Z_{l-1} W_V \\
A &= \text{softmax}\left(\frac{QK^T}{\sqrt{D_h}}\right) \\
Z'_l &= \text{LayerNorm}(Z_{l-1} + A V W_O) \\
Z_l &= \text{LayerNorm}(Z'_l + \text{FFN}(Z'_l))
\end{aligned}
$$

---

##  å…«ã€æœ€ç»ˆè¾“å‡ºä¸ä»»åŠ¡ç›®æ ‡

ç»è¿‡ $L$ å±‚ç¼–ç åï¼š

$$Z_L = \text{Encoder}(Z_0)$$

- è‹¥æ˜¯ **NLP**ï¼š  
  è¾“å‡ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„ç‰¹å¾å¯ç”¨äºç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
- è‹¥æ˜¯ **ViT**ï¼š  
  å– `[CLS]` ä½ç½®çš„å‘é‡ï¼š
  $$y = \text{Linear}(Z_L^{[CLS]})$$  
  ä½œä¸ºåˆ†ç±»ç»“æœã€‚

---

##  ä¹ã€Encoder çš„ç›®çš„æ€»ç»“è¡¨

| æ¨¡å— | æ•°å­¦å½¢å¼ | ç›®çš„ |
|------|-----------|------|
| Patch Embedding / Token Embedding | $X \to Z_0$ | å°†è¾“å…¥è½¬ä¸ºå‘é‡ |
| Positional Encoding | $Z_0 + E_{pos}$ | ä¿ç•™é¡ºåºä¿¡æ¯ |
| Q, K, V çº¿æ€§å˜æ¢ | $Q=ZW_Q, K=ZW_K, V=ZW_V$ | è¡¨å¾å…³æ³¨å…³ç³» |
| æ³¨æ„åŠ›æƒé‡ | $A = \text{softmax}(QK^T / \sqrt{D_h})$ | è®¡ç®—ç›¸å…³æ€§ |
| åŠ æƒæ±‚å’Œ | $AV$ | æ±‡èšä¸Šä¸‹æ–‡ä¿¡æ¯ |
| å¤šå¤´æ‹¼æ¥ | $[AV_1;\dots;AV_h]W_O$ | å¤šè§†è§’å»ºæ¨¡ |
| æ®‹å·® + Norm | $Z' = \text{LN}(Z+AVW_O)$ | ç¨³å®šæ¢¯åº¦ |
| FFN + æ®‹å·® | $Z = \text{LN}(Z'+\text{FFN}(Z'))$ | éçº¿æ€§è¡¨è¾¾ |

---


